
Improving Uncertainty Estimation with Neural Graphical Models
=============================================================

# Abstract


Building interpretable, composable autonomous systems requires consideration of uncertainties in the decisions and detections theygenerate. Human analysts need accurate absolute measures of probability to determine how to interpret and use the sometimes noisy resultsof machine learning systems; and composable autonomous systems need to be able to propagate uncertainties so that later reasoningsystems, or humans in the loop, can take them into consideration. For the task of detecting and labeling object groupings, we propose tocombine recent developments in graphical modeling using neural message passing and a comprehensive consideration of statistical sources ofuncertainty using recent developments in Bayesian variational inference for neural networks. Our system can make use of manually specifiedaggregation rules such as the linearity of a group of detections, and can learn unspecified aggregation rules of arbitrary complexity by its use ofdeep neural networks. Our system is also highly interpretable since it provides more than one measure of confidence, quantifying differentaspects of statistical certainty.  

# Award Details

|Branch|Award Year|Award Amount|Keywords|
| :---: | :---: | :---: | :---: |
|National Geospatial-Intelligence Agency|2018|$99,934|uncertainty estimation, graphical modeling, deep learning, interpretability, statistics, neural networks, group detection, activity detection|
  
  


[Back to Home](https://github.com/chrischow/dod_sbir_awards/JH/#2251)