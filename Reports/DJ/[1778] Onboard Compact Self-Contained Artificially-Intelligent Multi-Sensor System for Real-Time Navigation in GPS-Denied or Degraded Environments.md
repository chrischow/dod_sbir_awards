
Onboard Compact Self-Contained Artificially-Intelligent Multi-Sensor System for Real-Time Navigation in GPS-Denied or Degraded Environments
===========================================================================================================================================

# Abstract


Unmanned Aerial Systems (UASs) employed by the US Department of Defense are heavily reliant on accurate and available Positioning Navigation and Timing (PNT) to successfully execute their missions. The primary source of PNT is the Global Positioning System (GPS) which has been integrated in some fashion to the majority of autonomous systems currently in operation. While ubiquitous and highly useful, this system has issues in operational environments such as indoors, urban environments, mountainous or jungle terrains, or where line-of-site is not achievable by the receivers. Furthermore, in today’s Navigation Warfare (NAVWAR) environment, our peer and near-peer adversaries maintain the capability to degrade, disrupt, manipulate, and deny, access to Global Navigation Satellite Systems (GNSS). These negative actions result in reduced capability of autonomous systems to navigate and successfully execute their mission. Most UASs/ORBs have a suite of onboard sensors, including Electro-Optical (EO) cameras which provide real-time operational imagery which has the potential to be used to support autonomous position and navigation.  The current state of the art approach is visual odometry, which is a brute force approach to matching real-time imagery with known previously captured and geo-registered datasets.  However, this approach is extremely challenging and impractical due to computational complexity and high bandwidth transmission required to support the real-time image stream from the UAS.   The real-time high-fidelity imagery needs to be streamed to match against a large database of satellite imagery stored in a remote data-server facility. Such transmission requires ubiquitous, uninterrupted, and fast communication channels which will most likely not be available to the UAS. Moreover, matching of aerial images to satellite imagery using computer vision algorithms are known to be sensitive to illumination variations, protective deformations and changes in the scene.   Ubihere is proposing to overcome those challenges through the development of Ubivision, an compact, low-power, low-cost, artificially intelligent camera system.  By applying existing research from The Ohio State University, Ubihere will overcome these challenges and allow for real-time onboard navigation capability for UASs without the need for GPS or high-speed data connections.  The Ubihere proposed system will be capable of supplanting GPS in denied environments or enhancing it in degraded environments.  Ubihere is responding to the Air Force’s published areas of interest for application of dual-use technology to overcome Warfighter challenges. Specifically, the Air Force has published several Focus Areas relevant to the Agility Prime ORB efforts.  These areas include:  FA-03 “Tactical Level Innovation Opportunities”; FA-06 “Artificial Intelligence/Machine Learning Dual Use Technologies”; FA-15 “eVTOL/UAM (Electric Vertical Takeoff and Landing/Urban Air Mobility”  

# Award Details

|Branch|Award Year|Award Amount|Keywords|
| :---: | :---: | :---: | :---: |
|Air Force|2021|$149,996|assured pnt, position and navigation, autonomous navigation, gps denied, orbs, evtols, artificial intelligence, smart cameras|
  
  


[Back to Home](https://github.com/chrischow/dod_sbir_awards/Reports/DJ/#1778)