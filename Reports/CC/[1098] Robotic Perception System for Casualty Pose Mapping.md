
Robotic Perception System for Casualty Pose Mapping
===================================================

# Abstract


The future battlefield, whether in cluttered urban environments or isolated rural areas, will increasingly rely on unmanned systems to provide support to the warfighter. Medical unmanned systems must be able to reliably identify and interact with patients in order to perform casualty evacuations, execute telemedicine or medical interventions, or precisely communicate the location of all injured patients for further monitoring; a visual perception system is at the core of this requirement. The Areté Human Emergency Analysis and Determination (AHEAD) system is a vision module for unmanned systems; it will detect human figures, perform pose mapping and surface volume mapping, and communicate its 3D understanding to the unmanned system. The AHEAD module will be robust to common battlefield complications such as occlusions and body armor. During Phase I, Areté developed deep learning-driven algorithm for human perception by unmanned systems, training the algorithms on simulated data and demonstrating them on video of people in extreme poses and with occlusions. In Phase II, Areté will further develop the machine learning algorithms, build a hardware enclosure for processor, sensor, and communications interface, integrate the modular unit with a robotic system, and demonstrate the AHEAD unit in a field environment.  

# Award Details

|Branch|Award Year|Award Amount|Keywords|
| :---: | :---: | :---: | :---: |
|Army|2019|$972,668|deep learning, pose mapping, neural networks, machine vision, unmanned systems|
  
  


[Back to Home](https://github.com/chrischow/dod_sbir_awards#1098)